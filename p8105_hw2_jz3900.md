p8105_hw2_jz3900
================
ELisajava
2024-10-02

``` r
library(tidyverse)
library(readxl)
```

# Problem 1

## Step 1: Import and clean data

- Retain variables: line, station, name, station latitude, longitude,
  routes served, entry, vending, entrance type, and ADA compliance.

- Convert the entry variable from character to a logical variable : The
  entry column has YES or NO values, so you’ll need to convert it into a
  logical TRUE/FALSE format (the ifelse or case_match function may be
  useful).

``` r
# Read and clean the data
NYC_Subway_Data = read_csv("./dataset/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) %>% 
  mutate(entry = case_when(
    entry == "YES" ~ TRUE,
    entry == "NO"  ~ FALSE
  )) # Convert entry to logical TRUE/FALSE
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# View the cleaned data
NYC_Subway_Data
```

    ## # A tibble: 1,868 × 19
    ##    line     station_name station_latitude station_longitude route1 route2 route3
    ##    <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ##  1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  6 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  7 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  8 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  9 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ## 10 4 Avenue 53rd St                  40.6             -74.0 R      <NA>   <NA>  
    ## # ℹ 1,858 more rows
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <dbl>, route9 <dbl>, route10 <dbl>, route11 <dbl>, entry <lgl>,
    ## #   vending <chr>, entrance_type <chr>, ada <lgl>

### A description of the dataset:

The cleaned dataset includes variables such as subway line, station
name, latitude, longitude, routes served, entry status, vending,
entrance type, and ADA compliance. To prepare the data, I standardized
the column names using clean_names(), filtered relevant columns with
select(), and transformed the entry variable from character values
(“YES”/“NO”) to logical (TRUE/FALSE) using case_when(). The final
dataset consists of r nrow(NYC_Subway_Data) rows and r
ncol(NYC_Subway_Data) columns. It is tidy, with each variable as a
distinct column, each observation as a row, and each value neatly
contained in its own cell.

## Step 2: Answer the following questions bases on these data

### (i) How many distinct stations are there?

``` r
distinct_station = 
   NYC_Subway_Data %>%
  distinct(line, station_name) %>%
  nrow()
```

There are 465 distinct stations in this dataset.

### (ii) How many stations are ADA compliant?

``` r
ada_compliant =
  NYC_Subway_Data %>% 
  filter(ada == TRUE) %>% 
  distinct(line, station_name) %>% 
  nrow()
```

There are 84 ADA compliant stations.

### (iii) What proportion of station entrances / exits without vending allow entrance?

``` r
proportion_no_vending_allows_entry = 
  NYC_Subway_Data %>%
  filter(vending == "NO") %>%  # Filter rows where there is no vending
  summarise(proportion = mean(entry == TRUE)) %>%  # Calculate the proportion where entry is TRUE
  pull(proportion)

# Output the result
proportion_no_vending_allows_entry
```

    ## [1] 0.3770492

37.7% of station entrances / exits without vending allow entrance

## Step 3: Split the data into separate route number and route name variables.

``` r
NYC_Subway_Data_reformed = 
  NYC_Subway_Data %>%
  mutate(across(starts_with("route"), as.character)) %>%  # Ensure all route columns are characters
  pivot_longer(
    cols = starts_with("route"), # Specify the columns for the routes
    names_to = "route_number",
    values_to = "route") %>%
  drop_na(route)# Drop rows where route is NA
NYC_Subway_Data_reformed
```

    ## # A tibble: 4,270 × 10
    ##    line     station_name station_latitude station_longitude entry vending
    ##    <chr>    <chr>                   <dbl>             <dbl> <lgl> <chr>  
    ##  1 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ##  2 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ##  3 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  4 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  5 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  6 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  7 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  8 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  9 4 Avenue 45th St                  40.6             -74.0 TRUE  YES    
    ## 10 4 Avenue 45th St                  40.6             -74.0 TRUE  YES    
    ## # ℹ 4,260 more rows
    ## # ℹ 4 more variables: entrance_type <chr>, ada <lgl>, route_number <chr>,
    ## #   route <chr>

### (i) How many distinct stations serve the A train?

``` r
NYC_Subway_Data_reformed %>% 
  filter(route == "A") %>% 
  distinct(line, station_name) %>% 
  nrow()
```

    ## [1] 60

There are 60 distinct stations serve the A train.

### (ii) How many are ADA compliant of the stations that serve the A train?

``` r
NYC_Subway_Data_reformed %>% 
  filter(ada == TRUE) %>%  
  filter(route == "A") %>% 
  distinct(line, station_name) %>% 
  nrow()
```

    ## [1] 17

Among the 60 stations that serve the A train, 17 have at least one ADA
compliant entry or exit.

# Problem 2

## Step 1: Read and clean the Mr. Trash Wheel sheet

``` r
# Indicate the sheet and exclude non-data entries by skipping rows and specifying the range.
mr_trash_wheel = read_excel("./dataset/202409\ Trash\ Wheel\ Collection\ Data.xlsx",
                            sheet = "Mr. Trash Wheel", 
                            skip = 1,  # Skip the first row (it is non-data or a note)
                            range = "A2:N653")  # Adjust the range based on the actual data layout
mr_trash_wheel_cleaned = mr_trash_wheel %>% # Omit rows which do not include dumpster-specific data
  filter(!is.na(Dumpster)) %>%  # Retain only the rows where the 'dumpster' column is not NA
  janitor::clean_names() %>%
  select(dumpster:homes_powered) %>% 
  mutate(sports_balls = round(sports_balls, 0)) %>% 
  mutate(sports_balls = as.integer(sports_balls)) %>% 
  mutate(year = as.numeric(year)) %>% 
  mutate(trash_wheel = "mr_trash")
mr_trash_wheel_cleaned
```

    ## # A tibble: 651 × 15
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 641 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

## Step 2: Read and clean the Professor Trash Wheel sheet

``` r
prof_trash_wheel = 
  read_excel("./dataset/202409\ Trash\ Wheel\ Collection\ Data.xlsx",
             sheet = "Professor Trash Wheel",
             skip = 1,
             range = "A2:M121",) %>%
  janitor::clean_names() %>% 
  select(dumpster:homes_powered) %>%
  mutate(year = as.numeric(year)) %>% 
  mutate(trash_wheel = "pro_trash")

prof_trash_wheel 
```

    ## # A tibble: 119 × 14
    ##    dumpster month     year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>    <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 January   2017 2017-01-02 00:00:00        1.79                 15
    ##  2        2 January   2017 2017-01-30 00:00:00        1.58                 15
    ##  3        3 February  2017 2017-02-26 00:00:00        2.32                 18
    ##  4        4 February  2017 2017-02-26 00:00:00        3.72                 15
    ##  5        5 February  2017 2017-02-28 00:00:00        1.45                 15
    ##  6        6 March     2017 2017-03-30 00:00:00        1.71                 15
    ##  7        7 April     2017 2017-04-01 00:00:00        1.82                 15
    ##  8        8 April     2017 2017-04-20 00:00:00        2.37                 15
    ##  9        9 May       2017 2017-05-10 00:00:00        2.64                 15
    ## 10       10 May       2017 2017-05-26 00:00:00        2.78                 15
    ## # ℹ 109 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>, trash_wheel <chr>

## Step 3: Read and clean the Gwynnda Trash Wheel sheet

``` r
gwynnda_df = 
  read_excel("./dataset/202409\ Trash\ Wheel\ Collection\ Data.xlsx", 
             sheet = "Gwynnda Trash Wheel",
             skip = 1,
             range = "A2:L265", ) %>% 
  janitor::clean_names() %>% 
  select(dumpster:homes_powered) %>%
  mutate(trash_wheel = "gwynnda_trash") %>% 
  mutate(year = as.numeric(year))
gwynnda_df
```

    ## # A tibble: 263 × 13
    ##    dumpster month   year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 July    2021 2021-07-03 00:00:00        0.93                 15
    ##  2        2 July    2021 2021-07-07 00:00:00        2.26                 15
    ##  3        3 July    2021 2021-07-07 00:00:00        1.62                 15
    ##  4        4 July    2021 2021-07-16 00:00:00        1.76                 15
    ##  5        5 July    2021 2021-07-30 00:00:00        1.53                 15
    ##  6        6 August  2021 2021-08-11 00:00:00        2.06                 15
    ##  7        7 August  2021 2021-08-14 00:00:00        1.9                  15
    ##  8        8 August  2021 2021-08-16 00:00:00        2.16                 15
    ##  9        9 August  2021 2021-08-16 00:00:00        2.6                  15
    ## 10       10 August  2021 2021-08-17 00:00:00        3.21                 15
    ## # ℹ 253 more rows
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>, trash_wheel <chr>

## Step 4: Combine the datasets

``` r
combined_trash_wheel_data = bind_rows(mr_trash_wheel_cleaned,
                                       prof_trash_wheel,
                                       gwynnda_df)
combined_trash_wheel_data
```

    ## # A tibble: 1,033 × 15
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 1,023 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

### A description of the dataset:

The resulting dataset contains 1033 observations, which include
information on the names of trash wheels, as well as the weight and type
of crash collected by each trash wheel.

``` r
total_weight_prof_trash_wheel = combined_trash_wheel_data %>%
  filter(trash_wheel == "pro_trash") %>%
  summarise(total_weight = sum(weight_tons, na.rm = TRUE)) %>%
  pull(total_weight)
```

The total weight of trash collected by Professor Trash Wheel is 246.74
tons.

``` r
total_cig_butts_gwynnda_june_2022 = combined_trash_wheel_data %>%
  filter(trash_wheel == "gwynnda_trash", month == "June", year == 2022) %>%
  summarise(total_cig_butts = sum(cigarette_butts, na.rm = TRUE)) %>%
  pull(total_cig_butts)
```

The total number of cigarette butts collected by Gwynnda in June of 2022
is 1.812^{4}.

# Problem 3

## Step 1: Data Wrangling

``` r
# Clean and split baker names, addressing instances where bakers have multiple names.
bakers_df =
  read_csv("./dataset/gbb_datasets/bakers.csv", 
           na = c("NA", "", ".")) %>%
  janitor::clean_names() %>%
  separate(baker_name, into = c("first_name", "last_name"), 
           sep = " ", 
           extra = "merge",  # To make sure the middle names are included in 'last_name'.
           fill = "right")  # If a last name is missing, it will be filled with NA.
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Import and clean bakes data
bakes_df = read_csv("./dataset/gbb_datasets/bakes.csv", 
                    na = c("NA", "", ".")) %>%
  janitor::clean_names()
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Import and clean the results data, skipping two rows if necessary
results_df = read_csv("./dataset/gbb_datasets/results.csv", 
                      skip = 2, 
                      na = c("NA", "", ".")) %>%
  janitor::clean_names()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Check column names in both datasets
colnames(bakes_df)
```

    ## [1] "series"         "episode"        "baker"          "signature_bake"
    ## [5] "show_stopper"

``` r
colnames(bakers_df)
```

    ## [1] "first_name"       "last_name"        "series"           "baker_age"       
    ## [5] "baker_occupation" "hometown"

``` r
colnames(results_df)
```

    ## [1] "series"    "episode"   "baker"     "technical" "result"

## Step 2: Clean every dataset

- Check for missing values: Use summary() or is.na() in R to check for
  missing data.

- Check for consistency across datasets: use `anti_join()` in R to
  detect discrepancies, such as missing matches between bakers and
  results.

### Sub-Step 1: Use anti_join() to check dataset matching and completeness.

``` r
# Combine first_name and last_name into a single 'baker' column in bakers_df
bakers_df = bakers_df %>%
  mutate(baker = paste(first_name, last_name))

# Check for unmatched rows using correct 'by' syntax
unmatched_bakes = anti_join(bakes_df, bakes_df, by = c("baker", "series"))
unmatched_results = anti_join(results_df, bakers_df, by = c("baker", "series"))
unmatched_bakers = anti_join(bakers_df, results_df, by = c("baker", "series"))

# View unmatched rows (if any)
unmatched_bakes
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>

``` r
unmatched_results
```

    ## # A tibble: 1,136 × 5
    ##    series episode baker     technical result
    ##     <dbl>   <dbl> <chr>         <dbl> <chr> 
    ##  1      1       1 Annetha           2 IN    
    ##  2      1       1 David             3 IN    
    ##  3      1       1 Edd               1 IN    
    ##  4      1       1 Jasminder        NA IN    
    ##  5      1       1 Jonathan          9 IN    
    ##  6      1       1 Louise           NA IN    
    ##  7      1       1 Miranda           8 IN    
    ##  8      1       1 Ruth             NA IN    
    ##  9      1       1 Lea              10 OUT   
    ## 10      1       1 Mark             NA OUT   
    ## # ℹ 1,126 more rows

``` r
unmatched_bakers
```

    ## # A tibble: 120 × 7
    ##    first_name last_name   series baker_age baker_occupation       hometown baker
    ##    <chr>      <chr>        <dbl>     <dbl> <chr>                  <chr>    <chr>
    ##  1 Ali        Imdad            4        25 Charity worker         Saltley… Ali …
    ##  2 Alice      Fevronia        10        28 Geography teacher      Essex    Alic…
    ##  3 Alvin      Magallanes       6        37 Nurse                  Brackne… Alvi…
    ##  4 Amelia     LeBruin         10        24 Fashion designer       Halifax  Amel…
    ##  5 Andrew     Smyth            7        25 Aerospace engineer     Derby /… Andr…
    ##  6 Annetha    Mills            1        30 Midwife                Essex    Anne…
    ##  7 Antony     Amourdoux        9        30 Banker                 London   Anto…
    ##  8 Beca       Lyne-Pirkis      4        31 Military Wives' Choir… Aldersh… Beca…
    ##  9 Ben        Frazer           2        31 Graphic Designer       Northam… Ben …
    ## 10 Benjamina  Ebuehi           7        23 Teaching assistant     South L… Benj…
    ## # ℹ 110 more rows

### Sub-Step 2: Standardize baker names by resolving “Joanne” to “Jo.”

The result of the `anti_join()` indicated that the names “Joanne” and
“Jo” do not match, although they belong to the same series. We will
standardize the name to “Jo” in both datasets, assuming they refer to
the same person.

``` r
# Standardize the 'baker' name from 'Joanne' to 'Jo' in results_df
results_df = results_df %>%
  mutate(baker = ifelse(baker == "Joanne", "Jo", baker))

# Remove extra quotes around the name 'Jo' in bakes_df if necessary
bakes_df = bakes_df %>%
  mutate(baker = ifelse(baker == '"Jo"', "Jo", baker))  # Standardize 'Jo' format
```

### Sub-Step 3: Verify Completeness After Correcting Baker Names

After standardizing the names (for example, renaming “Joanne” to “Jo”),
we will recheck the datasets to confirm that no mismatches remain
between the bakes_df, results_df, and bakers_df datasets.

``` r
# Recheck for any unmatched rows after correcting the names
unmatched_bakes = anti_join(bakes_df, bakers_df, by = c("baker", "series"))
unmatched_results = anti_join(results_df, bakers_df, by = c("baker", "series"))
unmatched_bakers = anti_join(bakers_df, results_df, by = c("baker", "series"))

# Display unmatched rows (if any exist)
unmatched_bakes
```

    ## # A tibble: 548 × 5
    ##    series episode baker     signature_bake                          show_stopper
    ##     <dbl>   <dbl> <chr>     <chr>                                   <chr>       
    ##  1      1       1 Annetha   "Light Jamaican Black Cakewith Strawbe… Red, White …
    ##  2      1       1 David     "Chocolate Orange Cake"                 Black Fores…
    ##  3      1       1 Edd       "Caramel Cinnamon and Banana Cake"      N/A         
    ##  4      1       1 Jasminder "Fresh Mango and Passion Fruit Humming… N/A         
    ##  5      1       1 Jonathan  "Carrot Cake with Lime and Cream Chees… Three Tiere…
    ##  6      1       1 Lea       "Cranberry and Pistachio Cakewith Oran… Raspberries…
    ##  7      1       1 Louise    "Carrot and Orange Cake"                Never Fail …
    ##  8      1       1 Mark      "Sticky Marmalade Tea Loaf"             Heart-shape…
    ##  9      1       1 Miranda   "Triple Layered Brownie Meringue Cake\… Three Tiere…
    ## 10      1       1 Ruth      "Three Tiered Lemon Drizzle Cakewith F… Classic Cho…
    ## # ℹ 538 more rows

``` r
unmatched_results
```

    ## # A tibble: 1,136 × 5
    ##    series episode baker     technical result
    ##     <dbl>   <dbl> <chr>         <dbl> <chr> 
    ##  1      1       1 Annetha           2 IN    
    ##  2      1       1 David             3 IN    
    ##  3      1       1 Edd               1 IN    
    ##  4      1       1 Jasminder        NA IN    
    ##  5      1       1 Jonathan          9 IN    
    ##  6      1       1 Louise           NA IN    
    ##  7      1       1 Miranda           8 IN    
    ##  8      1       1 Ruth             NA IN    
    ##  9      1       1 Lea              10 OUT   
    ## 10      1       1 Mark             NA OUT   
    ## # ℹ 1,126 more rows

``` r
unmatched_bakers
```

    ## # A tibble: 120 × 7
    ##    first_name last_name   series baker_age baker_occupation       hometown baker
    ##    <chr>      <chr>        <dbl>     <dbl> <chr>                  <chr>    <chr>
    ##  1 Ali        Imdad            4        25 Charity worker         Saltley… Ali …
    ##  2 Alice      Fevronia        10        28 Geography teacher      Essex    Alic…
    ##  3 Alvin      Magallanes       6        37 Nurse                  Brackne… Alvi…
    ##  4 Amelia     LeBruin         10        24 Fashion designer       Halifax  Amel…
    ##  5 Andrew     Smyth            7        25 Aerospace engineer     Derby /… Andr…
    ##  6 Annetha    Mills            1        30 Midwife                Essex    Anne…
    ##  7 Antony     Amourdoux        9        30 Banker                 London   Anto…
    ##  8 Beca       Lyne-Pirkis      4        31 Military Wives' Choir… Aldersh… Beca…
    ##  9 Ben        Frazer           2        31 Graphic Designer       Northam… Ben …
    ## 10 Benjamina  Ebuehi           7        23 Teaching assistant     South L… Benj…
    ## # ℹ 110 more rows

## Step 3: Merge the datasets

To merge the `results_df`, `bakes_df`, and `bakers_df` datasets into a
single unified dataset, we carry out consecutive `left_join()`
operations. The resulting dataset is subsequently reorganized and saved
as a CSV file.

``` r
# Merge results_df, bakes_df, and bakers_df into one final dataset
final_df <- results_df %>%
  left_join(bakes_df, by = c("baker", "series", "episode")) %>%
  left_join(bakers_df, by = c("baker", "series")) %>%
  relocate(series, episode, .before = "baker")  # Reorder columns for better readability

# Export the final dataset as a CSV file
write.csv(final_df, file = "./dataset/gbb_datasets/final.csv")
```

### A description of the dataset:

Upon reviewing the raw data, I found that while bakers.csv contains the
full names of the bakers, both bakes.csv and results.csv only list the
first names. To address this, I split the full names in bakers.csv into
first and last names. Additionally, I noticed that the first two rows of
results.csv contained unnecessary information, so I skipped them to
simplify processing.

Next, I used the anti_join function to identify any inconsistencies
between the datasets and resolved the mismatches, particularly with the
baker names. After ensuring all data was aligned, I merged the datasets
in a way that retained all relevant information. I also reorganized the
variables for better clarity, placing the series and episode information
before the baker’s name.

Finally, I saved the merged dataset in the gbb_dataset directory. The
resulting dataset combines all the information from bakers.csv,
bakes.csv, and results.csv without losing any data. The variables are
arranged logically, making it easy for users to quickly locate bakers
based on the series and episode.

## Step 4: Create table showing winner or star baker

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10.

``` r
# Filter results for seasons 5 to 10 and include only winners and star bakers
winners_df <- results_df %>%
  filter(between(series, 5, 10), result %in% c("WINNER", "STAR BAKER")) %>%
  select(series, episode, baker, result)

# Pivot the data and arrange by episode, then display it in a table format
winners_df %>%
  pivot_wider(names_from = series, values_from = baker) %>%
  arrange(episode) %>%
  knitr::kable()
```

| episode | result     | 5       | 6      | 7         | 8      | 9       | 10       |
|--------:|:-----------|:--------|:-------|:----------|:-------|:--------|:---------|
|       1 | STAR BAKER | Nancy   | Marie  | Jane      | Steven | Manon   | Michelle |
|       2 | STAR BAKER | Richard | Ian    | Candice   | Steven | Rahul   | Alice    |
|       3 | STAR BAKER | Luis    | Ian    | Tom       | Julia  | Rahul   | Michael  |
|       4 | STAR BAKER | Richard | Ian    | Benjamina | Kate   | Dan     | Steph    |
|       5 | STAR BAKER | Kate    | Nadiya | Candice   | Sophie | Kim-Joy | Steph    |
|       6 | STAR BAKER | Chetna  | Mat    | Tom       | Liam   | Briony  | Steph    |
|       7 | STAR BAKER | Richard | Tamal  | Andrew    | Steven | Kim-Joy | Henry    |
|       8 | STAR BAKER | Richard | Nadiya | Candice   | Stacey | Ruby    | Steph    |
|       9 | STAR BAKER | Richard | Nadiya | Andrew    | Sophie | Ruby    | Alice    |
|      10 | WINNER     | Nancy   | Nadiya | Candice   | Sophie | Rahul   | David    |

From the results, we observe that each episode of every season features
a baker being awarded “STAR BAKER,” and at the end of each season, a
“WINNER” is crowned. It seems logical to assume that the more often a
baker is named “STAR BAKER,” the higher their chances of becoming the
season’s “WINNER.” This pattern holds for Nadiya in season 6, Candice in
season 7, Sophie in season 8, and Rahul in season 9. However, the
winners of seasons 5 and 10 did not follow this trend.

## Step 5: Import the views dataset

``` r
# Load and clean viewership data, reshaping it for analysis
viewership_df =
  read_csv("./dataset/gbb_datasets/viewers.csv") %>%
  janitor::clean_names() %>%
  gather(key = "series", value = "viewership", series_1:series_10) %>%
  mutate(
    series = as.numeric(gsub("series_", "", series))  # Extract series number and convert to numeric
  ) %>%
  arrange(series) %>%
  select(series, everything())  # Ensure 'series' is the first column
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
knitr::kable(head(viewership_df, 10), caption = "First 10 rows in the Viewership dataset")
```

| series | episode | viewership |
|-------:|--------:|-----------:|
|      1 |       1 |       2.24 |
|      1 |       2 |       3.00 |
|      1 |       3 |       3.00 |
|      1 |       4 |       2.60 |
|      1 |       5 |       3.03 |
|      1 |       6 |       2.75 |
|      1 |       7 |         NA |
|      1 |       8 |         NA |
|      1 |       9 |         NA |
|      1 |      10 |         NA |

First 10 rows in the Viewership dataset

I imported the data, cleaned the variable names, and organized it using
pivot_longer. The dataset now has a series column representing each
season and a viewership column containing the corresponding viewership
data. I removed the “series\_” prefix to convert the series numbers into
numeric values and then arranged the data in ascending order by series.
The first ten rows of this dataset are shown above. The average
viewership in series 1 is 2.77. The average viewership in series 5 is
10.04.
